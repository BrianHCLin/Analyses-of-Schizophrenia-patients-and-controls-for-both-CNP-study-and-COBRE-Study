---
title: "Stats 141 Final Project"
author: "Tsz Him Brian Ng"
date: "2/10/2018"
fontsize: 11pt
geometry: margin = 1.2in
header-includes:
- \usepackage{placeins}
- \usepackage{setspace}
- \usepackage{chngcntr}
- \usepackage{fancyhdr}
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{hyperref}
- \usepackage{enumitem}
- \onehalfspacing
- \counterwithin{table}{section}
output: pdf_document
---
\setlength{\headsep}{1cm}

\pagenumbering{gobble}
\tableofcontents
\newpage

\pagestyle{fancy}
\pagenumbering{arabic} 
\fancyhead[LE,RO]{} \fancyhead[LO,RE]{} \renewcommand{\headrulewidth}{0.4pt} \renewcommand{\footrulewidth}{0pt}
\fancyhead[CO,CE]{Packages} \section*{Packages} \addcontentsline{toc}{section}{Packages}

```{r warning= FALSE, message= FALSE}
library(readxl, warn.conflicts = FALSE, quietly = TRUE)
library(stringr, warn.conflicts = FALSE, quietly = TRUE)
library(dplyr, warn.conflicts = FALSE, quietly = TRUE)
library(readr, warn.conflicts = FALSE, quietly = TRUE)
library(randomForestSRC, warn.conflicts = FALSE, quietly = TRUE)
library(ggplot2, warn.conflicts = FALSE, quietly = TRUE)
library(ggthemes, warn.conflicts = FALSE, quietly=TRUE)
library(caret, warn.conflicts = FALSE, quietly = TRUE)
library(tidyr, warn.conflicts = FALSE, quietly = TRUE)
library(scales, warn.conflicts = FALSE, quietly = TRUE)
library(data.table, warn.conflicts = FALSE, quietly = TRUE)
library(effects, warn.conflicts = FALSE, quietly = TRUE)
library(gridExtra, warn.conflicts = FALSE, quietly = TRUE)
library(ggRandomForests, warn.conflicts = FALSE, quietly = TRUE)
library(ROCR, warn.conflicts = FALSE, quietly=TRUE)
library(ggpubr, warn.conflicts = FALSE, quietly=TRUE)
library(grid, warn.conflicts = FALSE, quietly=TRUE)
library(maxent, warn.conflicts = FALSE, quietly = TRUE)
```

\newpage
\fancyhead[LE,RO]{} \fancyhead[LO,RE]{} \renewcommand{\headrulewidth}{0.4pt} \renewcommand{\footrulewidth}{0pt}
\fancyhead[CO,CE]{Functions} \section*{Functions} \addcontentsline{toc}{section}{Functions}

\subsection{AccuracyCutoffInfo}
```{r}
# Obtain the accuracy on the trainining and testing dataset.
# for cutoff value ranging from .4 to .8 ( with a .05 increase )
# @train   : your data.table or data.frame type training data ( assumes you have the predicted score in it ).
# @test    : your data.table or data.frame type testing data
# @predict : prediction's column name (assumes the same for training and testing set)
# @actual  : actual results' column name
# returns  : 1. data : a data.table with three columns.
#            		   each row indicates the cutoff value and the accuracy for the 
#            		   train and test set respectively.
# 			 2. plot : plot that visualizes the data.table

AccuracyCutoffInfo <- function( train, test, predict, actual )
{
  # change the cutoff value's range as you please 
  cutoff <- seq( .05, 1, by = .025 )
  
  accuracy <- lapply( cutoff, function(c)
  {
    train_prediction <- as.factor(as.numeric( train[[predict]] > c ))
    test_prediction <- as.factor(as.numeric( test[[predict]] > c ))
                                                                
    levels(train_prediction) <- c(levels(train[[actual]][1]),levels(train[[actual]])[2])
    levels(test_prediction) <- c(levels(test[[actual]][1]),levels(test[[actual]])[2])
    

    # use the confusionMatrix from the caret package
    cm_train <- confusionMatrix( train_prediction, train[[actual]] )
    cm_test  <- confusionMatrix( test_prediction, test[[actual]] )
    
    dt <- data.table( cutoff = c,
                      train  = cm_train$overall[["Accuracy"]],
                      test   = cm_test$overall[["Accuracy"]] )
    return(dt)
  }) %>% rbindlist()
  
  # visualize the accuracy of the train and test set for different cutoff value 
  # accuracy in percentage.
  accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
  
  plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) + 
    geom_line( size = 1 ) + geom_point( size = 3 ) +
    scale_y_continuous( label = percent ) +
    ggtitle( "Train/Test Accuracy for Different Cutoff" ) +
    scale_x_continuous(breaks=seq(0, 1, 0.1)) +
    theme_bw()
  
  return( list( data = accuracy, plot = plot ) )
}

```

\subsection{ConfusionMatrixInfo}
```{r}
ConfusionMatrixInfo <- function( data, predict, actual, cutoff )
{	
  # extract the column ;
  # relevel making 1 appears on the more commonly seen position in 
  # a two by two confusion matrix	
  predict <- data[[predict]]
  temp_data <- as.factor( as.numeric(data[[actual]]) )
  levels(temp_data) <- c(0,1)
  actual  <- relevel(temp_data, "1") 
  
  result <- data.table( actual = actual, predict = predict )
  
  # caculating each pred falls into which category for the confusion matrix
  result[ , type := ifelse( predict >= cutoff & actual == 1, "TP",
                            ifelse( predict >= cutoff & actual == 0, "FP", 
                                    ifelse( predict <  cutoff & actual == 1, "FN", "TN" ) ) ) %>% as.factor() ]
  
  # jittering : can spread the points along the x axis 
  plot <- ggplot( result, aes( actual, predict, color = type ) ) + 
    geom_violin( fill = "white", color = NA ) +
    geom_jitter( shape = 1 ) + 
    geom_hline( yintercept = cutoff, color = "blue", alpha = 0.6 ) + 
    scale_y_continuous( limits = c( 0, 1 ) ) + 
    scale_color_discrete( breaks = c( "TP", "FN", "FP", "TN" ) ) + # ordering of the legend 
    guides( col = guide_legend( nrow = 2 ) ) + # adjust the legend to have two rows  
    ggtitle( sprintf( "Confusion Matrix with Cutoff at %.2f", cutoff ) )
  
  return( list( data = result, plot = plot ) )
}
```

\subsection{ROCInfo}
```{r}
# Pass in the data that already consists the predicted score and actual outcome.
# to obtain the ROC curve 
# @data    : your data.table or data.frame type data that consists the column
#            of the predicted score and actual outcome
# @predict : predicted score's column name
# @actual  : actual results' column name
# @cost.fp : associated cost for a false positive 
# @cost.fn : associated cost for a false negative 
# return   : a list containing  
#			 1. plot        : a side by side roc and cost plot, title showing optimal cutoff value
# 				 	   		  title showing optimal cutoff, total cost, and area under the curve (auc)
# 		     2. cutoff      : optimal cutoff value according to the specified fp/fn cost 
#		     3. totalcost   : total cost according to the specified fp/fn cost
#			 4. auc 		: area under the curve
#		     5. sensitivity : TP / (TP + FN)
#		     6. specificity : TN / (FP + TN)

ROCInfo <- function( data, predict, actual, cost.fp, cost.fn )
{
  # calculate the values using the ROCR library
  # true positive, false postive 
  pred <- prediction( data[[predict]], data[[actual]] )
  perf <- performance( pred, "tpr", "fpr" )
  roc_dt <- data.frame( fpr = perf@x.values[[1]], tpr = perf@y.values[[1]] )
  
  # cost with the specified false positive and false negative cost 
  # false postive rate * number of negative instances * false positive cost + 
  # false negative rate * number of positive instances * false negative cost
  cost <- perf@x.values[[1]] * cost.fp * sum( data[[actual]] == 0 ) + 
    ( 1 - perf@y.values[[1]] ) * cost.fn * sum( data[[actual]] == 1 )
  
  cost_dt <- data.frame( cutoff = pred@cutoffs[[1]], cost = cost )
  
  # optimal cutoff value, and the corresponding true positive and false positive rate
  best_index  <- which.min(cost)
  best_cost   <- cost_dt[ best_index, "cost" ]
  best_tpr    <- roc_dt[ best_index, "tpr" ]
  best_fpr    <- roc_dt[ best_index, "fpr" ]
  best_cutoff <- pred@cutoffs[[1]][ best_index ]
  
  # area under the curve
  auc <- performance( pred, "auc" )@y.values[[1]]
  
  # normalize the cost to assign colors to 1
  normalize <- function(v) ( v - min(v) ) / diff( range(v) )
  
  # create color from a palette to assign to the 100 generated threshold between 0 ~ 1
  # then normalize each cost and assign colors to it, the higher the blacker
  # don't times it by 100, there will be 0 in the vector
  col_ramp <- colorRampPalette( c( "green", "orange", "red", "black" ) )(100)   
  col_by_cost <- col_ramp[ ceiling( normalize(cost) * 99 ) + 1 ]
  
  roc_plot <- ggplot( roc_dt, aes( fpr, tpr ) ) + 
    geom_line( color = rgb( 0, 0, 1, alpha = 0.3 ) ) +
    geom_point( color = col_by_cost, size = 4, alpha = 0.2 ) + 
    geom_segment( aes( x = 0, y = 0, xend = 1, yend = 1 ), alpha = 0.8, color = "royalblue" ) + 
    labs( title = "ROC", x = "False Postive Rate", y = "True Positive Rate" ) +
    geom_hline( yintercept = best_tpr, alpha = 0.8, linetype = "dashed", color = "steelblue4" ) +
    geom_vline( xintercept = best_fpr, alpha = 0.8, linetype = "dashed", color = "steelblue4" ) + 
    theme_bw()
  
  cost_plot <- ggplot( cost_dt, aes( cutoff, cost ) ) +
    geom_line( color = "blue", alpha = 0.5 ) +
    geom_point( color = col_by_cost, size = 4, alpha = 0.5 ) +
    ggtitle( "Cost" ) +
    scale_y_continuous( labels = comma ) +
    geom_vline( xintercept = best_cutoff, alpha = 0.8, linetype = "dashed", color = "steelblue4" ) + 
    theme_bw()
  
  # the main title for the two arranged plot
  sub_title <- sprintf( "Cutoff at %.2f - Total Cost = %.2f, AUC = %.3f", 
                        best_cutoff, best_cost, auc )
  
  # arranged into a side by side plot
  plot <- arrangeGrob( roc_plot, cost_plot, ncol = 2, 
                       top = textGrob( sub_title, gp = gpar( fontsize = 16, fontface = "bold" ) ) )
  
  return( list( plot 		  = plot, 
                cutoff 	  = best_cutoff, 
                totalcost   = best_cost, 
                auc         = auc,
                sensitivity = best_tpr, 
                specificity = 1 - best_fpr ) )
}


```


\subsection{delete dup}
```{r}
#Some varaibles are forced into the model regardless of variable section result
#If the forced variable ended up being selected, this model will removed the duplicated variable. 

delete_dup <- function(subset, data){
  remove <- c() 
  for(i in 1:length(subset)){
    result <- str_detect(subset[i],names(data))
    for(j in 1:length(result)){
      if(result[j]){
        remove <- c(remove,i) 
      }
    }
  }
  if(is.null(remove))
    return(subset)
  subset <- subset[-c(remove)]
  return(subset)
}
```

\subsection{Classify}
```{r}
#data = data file 
#Predition: predicted result
#response: The name of response variable 
#cut_off: probabilty cut off point

Classify <- function(data, prediction,response, cut_off ){
  for(i in 1:length(prediction)){
    if(prediction[i] < cut_off){
      prediction[i] <- levels(data[[response]])[1]
    } else{
      prediction[i] <- levels(data[[response]])[2]
    }
  }
  
  prediction <- as.factor(prediction)
  levels(prediction) <- c(levels(data[[response]])[1],levels(data[[response]])[2])
  confuseion_matrix <- table(data[[response]],prediction)
  print(confuseion_matrix)
  Accuracy <- (confuseion_matrix[1,1] + confuseion_matrix[2,2])/sum(confuseion_matrix)
  TPR <- confuseion_matrix[2,2] / (confuseion_matrix[2,2] + confuseion_matrix[2,1])
  return(cat(paste("The accuracy is", round(Accuracy*100,3),"%.\nThe True positive rate is", round(TPR*100,3),"%\n")))
}
```

\subsection{cv.error}
```{r}
#data = data using for prediction
#response = name of the response variable
#cut off = probability cut off point
#interaction = you can type addition interaction term in text
#Example 
#cv.error(CNP_logi_subset,"Subject_Type","+Age*Auditory.global_eff", 0.8)


cv.error <- function(data, response, interaction = "", cut_off = 0.5){
  
  #generate random seeds
  r <- runif(1,0,9999)
  set.seed(r)
  folds <- createFolds(data[[response]],k = 10)
  Accuracy <- rep(NA,10)
  TPR <- rep(NA,10)
  
  for(i in 1:10){
    
    #training and testing
    train <- data[-folds[[i]],]
    test <- data[folds[[i]],]
    
    levels(test[[response]]) <- c(levels(data[[response]])[1],levels(data[[response]])[2])
    
    logi_cv <-glm(paste(response,"~.",interaction), data = train, family = "binomial") 
    
    prediction <- predict(logi_cv, test, type = "response")
    for(j in 1:length(prediction)){
      if(prediction[j] < cut_off){
        prediction[j] <- levels(test[[response]])[1]
      } else{
        prediction[j] <- levels(test[[response]])[2]
      }
    }
    prediction <- as.factor(prediction)
    levels(prediction) <- c(levels(data[[response]])[1],levels(data[[response]])[2])
    
    confuseion_matrix <- table(test[[response]],prediction)
    Accuracy[i] <- (confuseion_matrix[1,1] + confuseion_matrix[2,2])/sum(confuseion_matrix)
    TPR[i] <- confuseion_matrix[2,2] / (confuseion_matrix[2,2] + confuseion_matrix[2,1])
  }
  return(list(Accuracy, TPR))
}
```

\subsection{Standarize}
```{r}
#Standardized variable

Standarize <- function(data){
  for(i in 1:ncol(data)){
    if(is.numeric(data[1,i])){
      data[,i] <- (data[,i] - mean(data[,i]))/sd(data[,i]) 
    }
  }
  return(data)
}
```

\fancyhead[LE,RO]{} \fancyhead[LO,RE]{} \renewcommand{\headrulewidth}{0.4pt} \renewcommand{\footrulewidth}{0pt}
\fancyhead[CO,CE]{Data Import} \section*{Data Import} \addcontentsline{toc}{section}{Data Import}
```{r}
#Load data

#load CNP data

CNP_between <- read.table("A:/Winter 2018/Stats 141SL/project/CNP_between_nets.txt", header =  TRUE)
CNP_within <- read.table("A:/Winter 2018/Stats 141SL/project/CNP_within_nets.txt", header = TRUE)
CNPDemographic <- read_excel("A:/Winter 2018/Stats 141SL/project/CNPDemographicMeasures.xlsx", sheet = "SNF")


#load COBRE data

COBRE_between <- read.table("A:/Winter 2018/Stats 141SL/project/COBRE_between_nets.txt", header = TRUE)
COBRE_within <- read.table("A:/Winter 2018/Stats 141SL/project/COBRE_within_nets.txt", header = TRUE)
COBREDemographic <- read_excel("A:/Winter 2018/Stats 141SL/project/COBRE INDI Additional data.xls", sheet = "NP")
COBRE_phenotypic <- read_csv("A:/Winter 2018/Stats 141SL/project/COBRE_phenotypic_data.csv")
```

\fancyhead[LE,RO]{} \fancyhead[LO,RE]{} \renewcommand{\headrulewidth}{0.4pt} \renewcommand{\footrulewidth}{0pt}
\fancyhead[CO,CE]{Data Cleaning} \section*{Data Cleaning} \addcontentsline{toc}{section}{Data Cleaning}

\subsection{CNP Data}

\subsubsection{Cleaning ID Column}
```{r}
# Removed character string

pattern <- "[a-z]*-"

CNP_within$Subject_ID <- as.numeric(str_replace_all(CNP_within$Subject_ID 
, pattern,""))

CNP_between$Subject_ID <- as.numeric(str_replace_all(CNP_between$Subject_ID 
, pattern,""))
```

\subsubsection{Merging Data With Demographic Data}
```{r}
CNP_within_merge <- left_join(CNP_within,CNPDemographic, by = c("Subject_ID" = "PTID"))

#summary(CNP_within_merge)

CNP_between_merge <- left_join(CNP_between,CNPDemographic, by = c("Subject_ID" = "PTID"))

#summary(CNP_between_merge)
```

\subsubsection{Dropping Unwanted Subject Types}
```{r}
CNP_within_merge <- CNP_within_merge %>%
  filter(Subject_Type == "Control" | Subject_Type == "Schizophrenia")

table(CNP_within_merge$Subject_Type)

CNP_between_merge <- CNP_between_merge %>%
  filter(Subject_Type == "Control" | Subject_Type == "Schizophrenia")

table(CNP_between_merge$Subject_Type)

CNP_within_merge$Subject_Type <- droplevels(CNP_within_merge$Subject_Type)
levels(CNP_within_merge$Subject_Type)

CNP_between_merge$Subject_Type <- droplevels(CNP_between_merge$Subject_Type)
levels(CNP_between_merge$Subject_Type)
```

\subsubsection{Merging Data into One Data}
```{r}
#CNP between
#remove 96:98, 112
CNP_between_merge <- CNP_between_merge %>%
  select(-c(96:98,112))

#CNP within get rid of
#75 #76 #91
CNP_within_merge <- CNP_within_merge %>%
  select(-c(75:77,91))

#Merge both between and within data into CNP

CNP <- merge(CNP_between_merge,CNP_within_merge, all = TRUE)

CNP_RF_subset <- CNP %>%
  select(-c(1,5:41))
```

\subsection{COBRE Data}

\subsubsection{Cleaning ID Column}
```{r}
#Revmove character string 

COBRE_between$Subject_ID <- as.numeric(str_replace_all(COBRE_between$Subject_ID 
, pattern,""))

COBRE_within$Subject_ID <- as.numeric(str_replace_all(COBRE_within$Subject_ID 
, pattern,""))

#remove 00

pattern <- "^00"

COBREDemographic$ID <- as.numeric(str_replace_all(COBREDemographic$ID, pattern,""))
```

\subsubsection{Cleaning Phenotypic Data}
```{r}
COBRE_phenotypic$Gender <- as.factor(COBRE_phenotypic$Gender)

COBRE_phenotypic <- COBRE_phenotypic %>%
  filter(!(COBRE_phenotypic$Gender == "Disenrolled"))

COBRE_phenotypic$Gender <- droplevels(COBRE_phenotypic$Gender)

colnames(COBRE_phenotypic)[1:2] <- c("Subject_ID", "Age")
```

\subsubsection{Merging Data}
```{r}
COBRE_within_merge <- left_join(COBRE_within,COBREDemographic, by = c("Subject_ID" = "ID"))

#summary(COBRE_within_merge)

COBRE_between_merge <- left_join(COBRE_between,COBREDemographic, by = c("Subject_ID" = "ID"))

#summary(COBRE_between_merge)

COBRE_between_merge <- merge(COBRE_between_merge,COBRE_phenotypic, all = TRUE)
COBRE_within_merge <- merge(COBRE_within_merge,COBRE_phenotypic, all = TRUE)

table(COBRE_between_merge$Diagnosis)
table(COBRE_within_merge$Diagnosis)
```

\subsubsection{Mapping and Dropping Unwanted Subject Types}
```{r}
COBRE_between_merge <- COBRE_between_merge %>%
  filter(!(Diagnosis == 290.3 | Diagnosis == 296.26 | Diagnosis == 296.4 | Diagnosis == 311))

COBRE_within_merge <- COBRE_within_merge %>%
  filter(!(Diagnosis == 290.3 | Diagnosis == 296.26 | Diagnosis == 296.4 | Diagnosis == 311))

table(COBRE_between_merge$Diagnosis)
table(COBRE_within_merge$Diagnosis)
```

\subsubsection{Recoding Patients to Schizophrenia}
```{r}
pattern <- "Patient"

COBRE_between_merge$Subject_Type <- str_replace_all(COBRE_between_merge$Subject_Type,  pattern,"Schizophrenia")
COBRE_within_merge$Subject_Type <- str_replace_all(COBRE_within_merge$Subject_Type,  pattern,"Schizophrenia")

table(COBRE_between_merge$Subject_Type)
table(COBRE_within_merge$Subject_Type)
```

\subsubsection{Merging Data into One Data}
```{r}
#Merge both between and within into COBRE 

COBRE <- merge(COBRE_between_merge, COBRE_within_merge, all = TRUE)

#Use only the fMRI, MRI, and Age, keep global EFF

COBRE_RF_subset<- COBRE %>%
  select(-c(1,5:111))

COBRE_RF_subset$Subject_Type <- as.factor(COBRE_RF_subset$Subject_Type)
```
\newpage
\fancyhead[LE,RO]{} \fancyhead[LO,RE]{} \renewcommand{\headrulewidth}{0.4pt} \renewcommand{\footrulewidth}{0pt}
\fancyhead[CO,CE]{Data Analysis} \section*{Data Analysis} \addcontentsline{toc}{section}{Data Analysis}

\subsection{CNP Data Modeling}
```{r, fig.width=10, fig.height=10}
#CNP data modeling 

set.seed(4321)

rfsrc_m1 <- rfsrc(as.factor(Subject_Type)~.,data = CNP_RF_subset, na.action = c("na.omit"), ntree= 1000)

max_var <- max.subtree(rfsrc_m1, conservative = TRUE)
max_var$topvars
#delete duplicate entity

#Logistic Regression Model

subset1 <- as.vector(max_var$topvars)

subset1 <- delete_dup(subset1,CNP_RF_subset[,c(1,137:150)])

CNP_logi_subset <- CNP_RF_subset[,c("Subject_Type",names(CNP_RF_subset[,c(1,137:150)]), subset1)]

#Using a previously grown forest, identify pairwise interactions for all pairs of variables from a specified list. There are two distinct approaches specified by the option method.

#method="maxsubtree"

#This invokes a maximal subtree analysis. 

CNP_logi_subset <- na.omit(CNP_logi_subset) %>%
  Standarize()

#Find interaction
gg_int <- gg_interaction(find.interaction(rfsrc_m1,
                                          xvar.names = names(CNP_logi_subset[,-c(1)]),
                                          sorted = FALSE,
                                          verbose = FALSE))

plot(gg_int)

#Minimal depth variable interaction plot for all variables of interest. 
#Higher values indicate lower interactivity with target variable marked in red.

#No interactioin found base on the result, we don't have to add interaction term


#Correlation check
high_cor <- findCorrelation(cor(CNP_logi_subset[,-c(1:2)]),cutoff = 0.75) + 2

#No potential multicollinearity problem 

index <- sample(1:nrow(CNP_logi_subset), size = round(nrow(CNP_logi_subset)*0.7,0),replace = FALSE)

CNP_train <- CNP_logi_subset[index,]
CNP_test <- CNP_logi_subset[-index,]
logi_m1 <-glm(Subject_Type~. , data = CNP_train, family = "binomial") 

write.csv(CNP_logi_subset, file = "Data_CNP_Logi.csv", row.names = FALSE)
save.model(logi_m1, file = "logistic_model_cnp.RData")

summary(logi_m1)
round(exp(coef(logi_m1)),3)
anova(logi_m1, test = "Chisq")

#R-squared

R_squared <- 1 - (summary(logi_m1)[[4]]/summary(logi_m1)[[8]])
R_squared

#70/30 CV check

#Train
CNP_train$prediction <- predict(logi_m1, CNP_train, type = "response")


#Test
CNP_test$prediction <- predict(logi_m1, CNP_test, type = "response")

prop.table(table(CNP$Subject_Type))

accuracy_info <- AccuracyCutoffInfo( train = CNP_train, test = CNP_test, 
                                     predict = "prediction", actual = "Subject_Type" )

accuracy_info$plot


Classify(CNP_train, CNP_train$prediction,"Subject_Type", 0.75 )

Classify(CNP_test, CNP_test$prediction,"Subject_Type", 0.75 )

set.seed(4321)
#CNP ROC search for better True positive rate. 

#cutoff : Optimal cutoff value according to the specified FP and FN cost .
#totalcost : Total cost according to the specified FP and FN cost.
#auc : Area under the curve.
#sensitivity : TP / (TP + FN) for the optimal cutoff.
#specificity : TN / (FP + TN) for the optimal cutoff.


cm_info <- ConfusionMatrixInfo(data = CNP_test, predict = "prediction", actual = "Subject_Type", 0.75)

cm_info$plot
invisible(dev.off())

roc_info <- ROCInfo( data = cm_info$data, predict = "predict", 
                     actual = "actual", cost.fp = 1000, cost.fn = 1500 )

#Optimal cutoff for True positive rate
roc_info$cutoff
```

```{r}
grid.draw(roc_info$plot)
```

```{r}
#CNP model k fold CV check
set.seed(4321)

#Optimal cutoff for Accuracy
result <- cv.error(CNP_logi_subset, "Subject_Type",cut_off = roc_info$cutoff)
Accuracy.k <- result[[1]]
mean(Accuracy.k)
TTP.k <- result[[2]]
mean(TTP.k)

#Optimal cutoff for True positive rate
result <- cv.error(CNP_logi_subset, "Subject_Type",cut_off = roc_info$cutoff)
Accuracy.k <- result[[1]]
mean(Accuracy.k)
TTP.k <- result[[2]]
mean(TTP.k)
```



\subsection{COBRE Data Modeling}
```{r, fig.width=10, fig.height=10}
set.seed(4321)

#Random Forest variable section 
rfsrc_m2 <- rfsrc(Subject_Type~.,data = COBRE_RF_subset, na.action = c("na.omit"), ntree= 1000)

max_var2 <- max.subtree(rfsrc_m2, conservative = TRUE)
max_var2$topvars
#delete duplicate entity

subset2 <- as.vector(max_var2$topvars)

subset2 <- delete_dup(subset2,COBRE_RF_subset[,c(1,137:150)])


#Logistic Regression model


COBRE_logi_subset <- COBRE_RF_subset[,c("Subject_Type",names(COBRE_RF_subset[,c(1,137:150)]), subset2)]


COBRE_logi_subset <- na.omit(COBRE_logi_subset) %>%
  Standarize()


#Find interaction
gg_int <- gg_interaction(find.interaction(rfsrc_m2, 
                                          xvar.names = names(COBRE_logi_subset[,-c(1)]), 
                                          sorted = FALSE,
                                          verbose = FALSE))
plot(gg_int)

#No interactioin fund base on the result, we don't have to add interaction term


#Correlation check
high_cor <- findCorrelation(cor(COBRE_logi_subset[,-c(1:2)]),cutoff = 0.75) + 2

#No potential multicollinearity problem


index <- sample(1:nrow(COBRE_logi_subset), size = round(nrow(COBRE_logi_subset)*0.7,0),replace = FALSE)

COBRE_train <- COBRE_logi_subset[index,]
COBRE_test <- COBRE_logi_subset[-index,]
logi_m2 <-glm(Subject_Type~. , data = COBRE_train, family = "binomial") 

write.csv(COBRE_logi_subset, file = "Data_COBRE_Logi.csv", row.names = FALSE)
save.model(logi_m2, file = "logistic_model_cobre.RData")

summary(logi_m2)
round(exp(coef(logi_m2)),3)
anova(logi_m2, test = "Chisq")


#R-squared
R_squared <- 1 - (summary(logi_m2)[[4]]/summary(logi_m2)[[8]])
R_squared

#70/30 CV check

#Train
COBRE_train$prediction <- predict(logi_m2, COBRE_train, type = "response")


#Test
COBRE_test$prediction <- predict(logi_m2, COBRE_test, type = "response")

prop.table(table(COBRE$Subject_Type))


accuracy_info <- AccuracyCutoffInfo( train = COBRE_train, test = COBRE_test, 
                                     predict = "prediction", actual = "Subject_Type" )

accuracy_info$plot


Classify(COBRE_train, COBRE_train$prediction,"Subject_Type", 0.425)

Classify(COBRE_test, COBRE_test$prediction,"Subject_Type", 0.425)

#COBRE ROC search for better True positive rate. 

#cutoff : Optimal cutoff value according to the specified FP and FN cost .
#totalcost : Total cost according to the specified FP and FN cost.
#auc : Area under the curve.
#sensitivity : TP / (TP + FN) for the optimal cutoff.
#specificity : TN / (FP + TN) for the optimal cutoff.


cm_info2 <- ConfusionMatrixInfo(data = COBRE_test, predict = "prediction", actual = "Subject_Type", 0.425)

cm_info2$plot
invisible(graphics.off())
roc_info2 <- ROCInfo( data = cm_info2$data, predict = "predict", 
                     actual = "actual", cost.fp = 1000, cost.fn = 1200 )

#Optimal cutoff for True positive rate
roc_info2$cutoff
```

```{r}
grid.draw(roc_info2$plot)
```

```{r}
#COBRE model k fold CV check

set.seed(4321)
#Optimal cutoff for Accuracy
result <-  cv.error(COBRE_logi_subset, "Subject_Type", cut_off = 0.425)
Accuracy.k <- result[[1]]
mean(Accuracy.k) 

TTP <- result[[2]]
mean(TTP)

#Optimal cutoff for True postitive rate
result <-  cv.error(COBRE_logi_subset, "Subject_Type", cut_off = roc_info$cutoff)
Accuracy.k <- result[[1]]
mean(Accuracy.k) 

TTP <- result[[2]]
mean(TTP)


#When we want optimize True positive rate, we gave up about 10% of accuracy.  

```

\subsection{Fitting Data into Model Based on the Other Study}
```{r}
set.seed(4321)
#Fit Data into model build base on other study to test how it handles data from different study


#Fit COBRE data into CNP Model 
Fit_COBRE_logi_subset <- COBRE_RF_subset[,c("Subject_Type",names(COBRE_RF_subset[,c(1,137:150)]), subset1)] %>%
  Standarize()

Fit_COBRE_test <- Fit_COBRE_logi_subset 

invisible(rm(Fit_COBRE_logi_subset))

Fit_COBRE_test$prediction <- predict(logi_m1, Fit_COBRE_test, type = "response")
Classify(Fit_COBRE_test, Fit_COBRE_test$prediction,"Subject_Type", 0.17 )


#Fit CNP data into COBRE model
Fit_CNP_logi_subset <- CNP_RF_subset[,c("Subject_Type",names(CNP_RF_subset[,c(1,137:150)]), subset2)] %>%
  Standarize()

Fit_CNP_test <- Fit_CNP_logi_subset

invisible(rm(Fit_CNP_logi_subset))

Fit_CNP_test$prediction <- predict(logi_m2, Fit_CNP_test, type = "response")
Classify(Fit_CNP_test, Fit_CNP_test$prediction,"Subject_Type", cut_off = 0.69 )


#When we introduce data from the other study, the both model has a a low testing accuracy.
#This hint us that the two studys are different.
```

\subsection{Combing CNP and COBRE Data}
```{r}
#Combine data 

#Further data cleaning to merge CNP and COBRE data 
Study <- rep("CNP",nrow(CNP))

CNP <- data.frame(CNP,Study)

CNP <- CNP %>% 
  select(-c(7:41))

colnames(CNP)[5:6] <- c("Ethnicity","Education")

levels(CNP$Gender) <- c("Female","Male")

Study <- rep("COBRE",nrow(COBRE))
COBRE <- data.frame(COBRE,Study)

COBRE <- COBRE %>%
  select(-c(5,8:111))

# CNP Ethinicty
#1=Hispanic origin
#2=Not of Hispanic origin

#COBRE Ethinicty
#Caucasian = 1
#African-American	= 2
#Hispanic	= 3

#Recoding required

table(COBRE$Ethnicity)

for(i in 1:length(COBRE$Ethnicity)){
  if(!is.na(COBRE$Ethnicity[i])){
    if(COBRE$Ethnicity[i] == 1 | COBRE$Ethnicity[i] == 2)
      COBRE$Ethnicity[i] <- 4
  }
}
COBRE$Ethnicity <- COBRE$Ethnicity - 2 

table(COBRE$Ethnicity)

Data <- merge(CNP,COBRE, all = TRUE) %>%
  select(-c(1))
Data$Ethnicity <- as.factor(Data$Ethnicity)
levels(Data$Ethnicity) <- c("Hispanic", "non-Hispanic")

write.csv(Data, file = "Stats_141_Combined_Data.csv", row.names = FALSE)
```

\subsection{Combined Data Modeling}
```{r, fig.width=10, fig.height=10}
set.seed(4321)

# Combine Data modeling


#Random Forest variable selection

rfsrc_m3 <- rfsrc(Study~.,data = Data, na.action = c("na.omit"), ntree= 1000)


max_var <- max.subtree(rfsrc_m3, conservative = TRUE)

max_var$topvars

#delete duplicate entity


subset3 <- as.vector(max_var$topvars)

subset3 <- delete_dup(subset3,Data[,c(1:5,139:152)])



#Logistic Regression model

Data_logi <- Data[,c("Study",names(Data[,c(1:5,139:152)]), subset3)]

Data_logi <- na.omit(Data_logi) %>%
  Standarize()

write.csv(Data_logi, file = "Stats_141_Combined_Data2.csv", row.names = FALSE)

#find interaction
gg_int <- gg_interaction(find.interaction(rfsrc_m3,
                                          xvar.names = names(Data_logi[,-c(1)]), 
                                          sorted = FALSE,
                                          verbose = FALSE))
plot(gg_int)

#No interactioin fund base on the result, we don't have to add interaction term

#check correlation

high_cor <- findCorrelation(cor(Data_logi[,-c(1,3:5)]),cutoff = 0.75) + 4

#Remove variables to prevent multicollinearity problem
Data_logi <- Data_logi %>%
  select(-c(high_cor))

index <- sample(1:nrow(Data_logi), size = round(nrow(Data_logi)*0.7,0),replace = FALSE)

Data_train <- Data_logi[index,]
Data_test <- Data_logi[-index,]
logi_m3 <-glm(Study~. + Subject_Type*Age , data = Data_train, family = "binomial") 

save.model(logi_m3, file = "logistic_model.RData")
summary(logi_m3)
round(exp(coef(logi_m3)),3)
anova(logi_m3, test = "Chisq")


#R-squared

R_squared <- 1 - (summary(logi_m3)[[4]]/summary(logi_m3)[[8]])
R_squared

#Effect plot
plot(Effect(c("Subject_Type", "Age"), logi_m3),ask = FALSE)


#70/30 CV check

#Train
Data_train$prediction <- predict(logi_m3, Data_train, type = "response")


#Test
Data_test$prediction <- predict(logi_m3, Data_test, type = "response")

prop.table(table(Data$Study))


accuracy_info <- AccuracyCutoffInfo( train = Data_train, test = Data_test, 
                                     predict = "prediction", actual = "Study" )

accuracy_info$plot


Classify(Data_train, Data_train$prediction,"Study", 0.55)

Classify(Data_test, Data_test$prediction,"Study", 0.55)


#Combine data model k fold CV check
set.seed(4321)

Accuracy.k <- cv.error(Data_logi, "Study", cut_off = 0.55)[[1]]
Accuracy.k
mean(Accuracy.k)
```

\subsection{Plots}
```{r}
par(mfrow = c(2,2))

ggplot(data = na.omit(Data), aes(x = Gender, fill = Study)) +
  geom_bar() +
  theme_bw()

ggplot(data = na.omit(Data), aes(x = Ethnicity, fill = Study)) +
  geom_bar() +
  theme_bw()

plot1 <- ggplot(data = na.omit(Data), aes(x = Study, y = Age)) +
  geom_boxplot(fill = "steelblue") +
  theme_bw() 

plot2 <- ggplot(data = na.omit(Data), aes(x = Age, fill = Study)) + 
  geom_density(alpha = 0.4) +
  theme_bw()

grid.arrange(plot1,plot2, nrow = 1, ncol = 2)

plot3 <- ggplot(data = na.omit(Data), aes(x = Study, y = Education)) +
  geom_boxplot(fill = "steelblue") +
  theme_bw()

plot4 <- ggplot(data = na.omit(Data), aes(x = Education, fill = Study)) +
  geom_density(alpha = 0.4) +
  theme_bw()

grid.arrange(plot3,plot4, nrow = 1, ncol = 2)
```

\subsection{Hypothesis Testings}
```{r}
#Recall the anova output for the combined data set logistic model

anova(logi_m3, test = "Chisq")

#Hypothesis testing for demographic variables in the combined data set


t.test(Age~Study, data = Data_logi)
t.test(Education~Study, data = Data_logi)



#Pearson's chi-squared test

#H_{0} = there is no difference between the distributions
#H_{1} = there is a difference between the distributions

chisq.test(table(Data_logi$Study, Data_logi$Gender))
chisq.test(table(Data_logi$Study, Data_logi$Ethnicity))
chisq.test(table(Data_logi$Study, Data_logi$Subject_Type))
```




