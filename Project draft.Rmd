---
title: "project"
author: "Brian Lin"
date: "February 14, 2018"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r warning= FALSE}
library(rJava, warn.conflicts = FALSE, quietly=TRUE)
library(xlsx, warn.conflicts = FALSE, quietly=TRUE)
library(stringr, warn.conflicts = FALSE, quietly=TRUE)
library(dplyr, warn.conflicts = FALSE, quietly=TRUE)
library(readr, warn.conflicts = FALSE, quietly=TRUE)
library(randomForestSRC, warn.conflicts = FALSE, quietly=TRUE)
library(ggplot2, warn.conflicts = FALSE, quietly=TRUE)
library(caret, warn.conflicts = FALSE, quietly=TRUE)
library(tidyr, warn.conflicts = FALSE, quietly=TRUE)
library(scales, warn.conflicts = FALSE, quietly=TRUE)
library(data.table, warn.conflicts = FALSE, quietly=TRUE)
library(effects, warn.conflicts = FALSE, quietly=TRUE )
```



```{r, warning = FALSE}
#Functions


#The AccuracyCutoffInfo function is a modified version of codes from the following github page
#https://github.com/ethen8181/machine-learning/blob/master/unbalanced/unbalanced_code/unbalanced_functions.R
#All Credit to user ethen8181


# ------------------------------------------------------------------------------------------
# [AccuracyCutoffInfo] : 
# Obtain the accuracy on the trainining and testing dataset.
# for cutoff value ranging from .4 to .8 ( with a .05 increase )
# @train   : your data.table or data.frame type training data ( assumes you have the predicted score in it ).
# @test    : your data.table or data.frame type testing data
# @predict : prediction's column name (assumes the same for training and testing set)
# @actual  : actual results' column name
# returns  : 1. data : a data.table with three columns.
#            		   each row indicates the cutoff value and the accuracy for the 
#            		   train and test set respectively.
# 			 2. plot : plot that visualizes the data.table

AccuracyCutoffInfo <- function( train, test, predict, actual )
{
  # change the cutoff value's range as you please 
  cutoff <- seq( .05, 1, by = .025 )
  
  accuracy <- lapply( cutoff, function(c)
  {
    train_prediction <- as.factor(as.numeric( train[[predict]] > c ))
    test_prediction <- as.factor(as.numeric( test[[predict]] > c ))
                                                                
    levels(train_prediction) <- c(levels(train[[actual]][1]),levels(train[[actual]])[2])
    levels(test_prediction) <- c(levels(test[[actual]][1]),levels(test[[actual]])[2])
    

    # use the confusionMatrix from the caret package
    cm_train <- confusionMatrix( train_prediction, train[[actual]] )
    cm_test  <- confusionMatrix( test_prediction, test[[actual]] )
    
    dt <- data.table( cutoff = c,
                      train  = cm_train$overall[["Accuracy"]],
                      test   = cm_test$overall[["Accuracy"]] )
    return(dt)
  }) %>% rbindlist()
  
  # visualize the accuracy of the train and test set for different cutoff value 
  # accuracy in percentage.
  accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
  
  plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) + 
    geom_line( size = 1 ) + geom_point( size = 3 ) +
    scale_y_continuous( label = percent ) +
    ggtitle( "Train/Test Accuracy for Different Cutoff" ) +
    scale_x_continuous(breaks=seq(0, 1, 0.1)) +
    theme_bw()
  
  return( list( data = accuracy, plot = plot ) )
}


#----------------------

#delete_dup

#Some varaibles are forced into the model regardless of variable section result
#If the forced variable ended up being selected, this model will removed the duplicated variable. 

delete_dup <- function(subset, data){
  remove <- c() 
  for(i in 1:length(subset)){
    result <- str_detect(subset[i],names(data))
    for(j in 1:length(result)){
      if(result[j]){
        remove <- c(remove,i) 
      }
    }
  }
  if(is.null(remove))
    return(subset)
  subset <- subset[-c(remove)]
  return(subset)
}


#data = data file 
#Predition: predicted result
#response: The name of response variable 
#cut_off: probabilty cut off point

Classify <- function(data, prediction,response, cut_off ){
  for(i in 1:length(prediction)){
    if(prediction[i] < cut_off){
      prediction[i] <- levels(data[[response]])[1]
    } else{
      prediction[i] <- levels(data[[response]])[2]
    }
  }
  
  prediction <- as.factor(prediction)
  levels(prediction) <- c(levels(data[[response]])[1],levels(data[[response]])[2])
  confuseion_matrix <- table(data[[response]],prediction)
  print(confuseion_matrix)
  Accuracy <- (confuseion_matrix[1,1] + confuseion_matrix[2,2])/sum(confuseion_matrix)
  return(print(paste("The accuracy is", round(Accuracy*100,3),"%")))
}



#K fold K = 10

#data = data using for prediction
#response = name of the response variable
#cut off = probability cut off point
#interaction = you can type addition interaction term in text
#Example 
#cv.error(CNP_logi_subset,"Subject_Type","+Age*Auditory.global_eff", 0.8)


cv.error <- function(data, response, interaction = "", cut_off = 0.5){
  
  #generate random seeds
  r <- runif(1,0,9999)
  set.seed(r)
  folds <- createFolds(data[[response]],k = 10)
  Accuracy <- rep(NA,10)
  
  for(i in 1:10){
    
    #training and testing
    train <- data[-folds[[i]],]
    test <- data[folds[[i]],]
    
    levels(test[[response]]) <- c(levels(data[[response]])[1],levels(data[[response]])[2])
    
    logi_cv <-glm(paste(response,"~.",interaction), data = train, family = "binomial") 
    
    prediction <- predict(logi_cv, test, type = "response")
    for(j in 1:length(prediction)){
      if(prediction[j] < cut_off){
        prediction[j] <- levels(test[[response]])[1]
      } else{
        prediction[j] <- levels(test[[response]])[2]
      }
    }
    prediction <- as.factor(prediction)
    levels(prediction) <- c(levels(data[[response]])[1],levels(data[[response]])[2])
    
    confuseion_matrix <- table(test[[response]],prediction)
    Accuracy[i] <- (confuseion_matrix[1,1] + confuseion_matrix[2,2])/sum(confuseion_matrix)
  }
  return(Accuracy)
}


#Standardized variable

Standarize <- function(Data){
  for(i in 1:ncol(Data)){
    if(is.numeric(Data[1,i])){
      Data[,i] <- (Data[,i] - mean(Data[,i]))/sd(Data[,i]) 
    }
  }
  return(Data)
}


```


```{r warning = FALSE}
#Load data

setwd("A:/Winter 2018/Stats 141SL/project/")

#load CNP data

CNP_between <- read.table("CNP_between_nets.txt", header =  TRUE)
CNP_within <- read.table("CNP_within_nets.txt", header = TRUE)
CNPDemographic <- read.xlsx("CNPDemographicMeasures.xlsx", sheetName = "SNF")


#load COBRE data


COBRE_between <- read.table("COBRE_between_nets.txt", header = TRUE)

COBRE_within <- read.table("COBRE_within_nets.txt", header = TRUE)

COBREDemographic <- read.xlsx("COBRE INDI Additional data.xls", sheetName = "NP")

COBRE_phenotypic <- read_csv("COBRE_phenotypic_data.csv")


```

```{r}
#Data cleaning process

#Removed character string

pattern <- "[a-z]*-"

CNP_within$Subject_ID <- as.numeric(str_replace_all(CNP_within$Subject_ID 
, pattern,""))

CNP_between$Subject_ID <- as.numeric(str_replace_all(CNP_between$Subject_ID 
, pattern,""))




#Merge data
CNP_within_merge <- left_join(CNP_within,CNPDemographic, by = c("Subject_ID" = "PTID"))

#summary(CNP_within_merge)

CNP_between_merge <- left_join(CNP_between,CNPDemographic, by = c("Subject_ID" = "PTID"))

#summary(CNP_between_merge)




#Revmove character string 

COBRE_between$Subject_ID <- as.numeric(str_replace_all(COBRE_between$Subject_ID 
, pattern,""))

COBRE_within$Subject_ID <- as.numeric(str_replace_all(COBRE_within$Subject_ID 
, pattern,""))




#remove 00

pattern <- "^00"

COBREDemographic$ID <- as.numeric(str_replace_all(COBREDemographic$ID, pattern,""))


#Merge data

COBRE_within_merge <- left_join(COBRE_within,COBREDemographic, by = c("Subject_ID" = "ID"))


#summary(COBRE_within_merge)

COBRE_between_merge <- left_join(COBRE_between,COBREDemographic, by = c("Subject_ID" = "ID"))

#summary(COBRE_between_merge)

COBRE_phenotypic$Gender <- as.factor(COBRE_phenotypic$Gender)

COBRE_phenotypic <- COBRE_phenotypic %>%
  filter(!(COBRE_phenotypic$Gender == "Disenrolled"))

COBRE_phenotypic$Gender <- droplevels(COBRE_phenotypic$Gender)

colnames(COBRE_phenotypic)[1:2] <- c("Subject_ID", "Age")

COBRE_between_merge <- merge(COBRE_between_merge,COBRE_phenotypic, all = TRUE)
COBRE_within_merge <- merge(COBRE_within_merge,COBRE_phenotypic, all = TRUE)


table(COBRE_between_merge$Diagnosis)
table(COBRE_within_merge$Diagnosis)



#CNP filter


CNP_within_merge <- CNP_within_merge %>%
  filter(Subject_Type == "Control" | Subject_Type == "Schizophrenia")

table(CNP_within_merge$Subject_Type)

  
CNP_between_merge <- CNP_between_merge %>%
  filter(Subject_Type == "Control" | Subject_Type == "Schizophrenia")

table(CNP_between_merge$Subject_Type)
  
  


#COBRE filter

COBRE_between_merge <- COBRE_between_merge %>%
  filter(!(Diagnosis == 290.3 | Diagnosis == 296.26 | Diagnosis == 296.4 | Diagnosis == 311))

COBRE_within_merge <- COBRE_within_merge %>%
  filter(!(Diagnosis == 290.3 | Diagnosis == 296.26 | Diagnosis == 296.4 | Diagnosis == 311))

table(COBRE_between_merge$Diagnosis)
table(COBRE_within_merge$Diagnosis)



#Recoding Patients to Schizophrenia in COBRE

pattern <- "Patient"

COBRE_between_merge$Subject_Type <- str_replace_all(COBRE_between_merge$Subject_Type,  pattern,"Schizophrenia")
COBRE_within_merge$Subject_Type <- str_replace_all(COBRE_within_merge$Subject_Type,  pattern,"Schizophrenia")

table(COBRE_between_merge$Subject_Type)
table(COBRE_within_merge$Subject_Type)


CNP_between_merge$Subject_Type <- droplevels(CNP_between_merge$Subject_Type)
levels(CNP_between_merge$Subject_Type)

CNP_within_merge$Subject_Type <- droplevels(CNP_within_merge$Subject_Type)
levels(CNP_within_merge$Subject_Type)



#CNP between
#remove 96:98, 112
CNP_between_merge <- CNP_between_merge %>%
  select(-c(96:98,112))



#CNP within get rid of
#75 #76 #91
CNP_within_merge <- CNP_within_merge %>%
  select(-c(75:77,91))


#Merge both data into CNP

CNP <- merge(CNP_between_merge,CNP_within_merge, all = TRUE)



#Use only the fMRI, MRI, and Age, keep global EFF

CNP_between_RF_subset <- CNP_between_merge %>%
  select(c(1:94))

CNP_within_RF_subset <- CNP_within_merge %>%
  select(c(1:72))

CNP_RF_subset <- CNP %>%
  select(-c(1,5:41))


```

```{r}
#CNP data modeling 

set.seed(4321)


rfsrc_m3 <- rfsrc(as.factor(Subject_Type)~.,data = CNP_RF_subset, na.action = c("na.omit"), ntree= 1000)

max_var <- max.subtree(rfsrc_m3, conservative = TRUE)
max_var$topvars
#delete duplicate entity


#Logistic Regression Model

subset <- as.vector(max_var$topvars)

subset <- delete_dup(subset,CNP_RF_subset[,c(1,137:150)])

CNP_logi_subset <- CNP_RF_subset[,c("Subject_Type",names(CNP_RF_subset[,c(1,137:150)]), subset)]



#Using a previously grown forest, identify pairwise interactions for all pairs of variables from a specified list. There are two distinct approaches specified by the option method.

#method="maxsubtree"

#This invokes a maximal subtree analysis. In this case, a matrix is returned where entries [i][i] are the normalized minimal depth of variable [i] relative to the root node (normalized wrt the size of the tree) and entries [i][j] indicate the normalized minimal depth of a variable [j] wrt the maximal subtree for variable [i] (normalized wrt the size of [i]'s maximal subtree). Smaller [i][i] entries indicate predictive variables. Small [i][j] entries having small [i][i] entries are a sign of an interaction between variable i and j (note: the user should scan rows, not columns, for small entries). See Ishwaran et al. (2010, 2011) for more details.

#method="vimp"

#This invokes a joint-VIMP approach. Two variables are paired and their paired VIMP calculated (refered to as 'Paired' importance). The VIMP for each separate variable is also calculated. The sum of these two values is refered to as 'Additive' importance. A large positive or negative difference between 'Paired' and 'Additive' indicates an association worth pursuing if the univariate VIMP for each of the paired-variables is reasonably large. See Ishwaran (2007) for more details.



#Find interaction
find.interaction(rfsrc_m3, xvar.names = names(CNP_logi_subset[,-c(1)]), sorted = FALSE)

#No interactioin fund base on the result, we don't have to add interaction term

CNP_logi_subset <- na.omit(CNP_logi_subset) %>%
  Standarize()



#Correlation check
high_cor <- findCorrelation(cor(CNP_logi_subset[,-c(1:2)]),cutoff = 0.75) + 2

#No potential multicollinearity problem 

index <- sample(1:nrow(CNP_logi_subset), size = round(nrow(CNP_logi_subset)*0.7,0),replace = FALSE)

CNP_train <- CNP_logi_subset[index,]
CNP_test <- CNP_logi_subset[-index,]
logi_m3 <-glm(Subject_Type~. , data = CNP_train, family = "binomial") 
summary(logi_m3)
round(exp(coef(logi_m3)),3)
anova(logi_m3, test = "Chisq")

#R-squared

R_squared <- 1 - (summary(logi_m3)[[4]]/summary(logi_m3)[[8]])
R_squared

#70/30 CV check

#Train
CNP_train$prediction <- predict(logi_m3, CNP_train, type = "response")


#Test
CNP_test$prediction <- predict(logi_m3, CNP_test, type = "response")

prop.table(table(CNP$Subject_Type))

accuracy_info <- AccuracyCutoffInfo( train = CNP_train, test = CNP_test, 
                                     predict = "prediction", actual = "Subject_Type" )

accuracy_info$plot


Classify(CNP_train, CNP_train$prediction,"Subject_Type", 0.75 )

Classify(CNP_test, CNP_test$prediction,"Subject_Type", 0.75 )

```



```{r warning = FALSE}
#CNP model k fold CV check
set.seed(1234)

Accuracy.k <- cv.error(CNP_logi_subset, "Subject_Type",cut_off = 0.75)
Accuracy.k
mean(Accuracy.k)


```



```{r warning = FALSE}
#COBRE data modeling 

set.seed(4321)

COBRE <- merge(COBRE_between_merge, COBRE_within_merge, all = TRUE)


COBRE_RF_subset<- COBRE %>%
  select(-c(1,5:111))

COBRE_RF_subset$Subject_Type <- as.factor(COBRE_RF_subset$Subject_Type)

#Random Forest variable section 
rfsrc_m4 <- rfsrc(Subject_Type~.,data = COBRE_RF_subset, na.action = c("na.omit"), ntree= 1000)


max_var <- max.subtree(rfsrc_m4, conservative = TRUE)
max_var$topvars
#delete duplicate entity


subset <- as.vector(max_var$topvars)

subset <- delete_dup(subset,COBRE_RF_subset[,c(1,137:150)])


#Logistic Regression model


COBRE_logi_subset <- COBRE_RF_subset[,c("Subject_Type",names(COBRE_RF_subset[,c(1,137:150)]), subset)]


#Find interaction
find.interaction(rfsrc_m4, xvar.names = names(COBRE_logi_subset[,-c(1)]), sorted = FALSE)

#No interactioin fund base on the result, we don't have to add interaction term


COBRE_logi_subset <- na.omit(COBRE_logi_subset) %>%
  Standarize()

#Correlation check
high_cor <- findCorrelation(cor(COBRE_logi_subset[,-c(1:2)]),cutoff = 0.75) + 2

#No potential multicollinearity problem


index <- sample(1:nrow(COBRE_logi_subset), size = round(nrow(COBRE_logi_subset)*0.7,0),replace = FALSE)

COBRE_train <- COBRE_logi_subset[index,]
COBRE_test <- COBRE_logi_subset[-index,]
logi_m4 <-glm(Subject_Type~. , data = COBRE_train, family = "binomial") 
summary(logi_m4)
round(exp(coef(logi_m4)),3)
anova(logi_m4, test = "Chisq")


#R-squared
R_squared <- 1 - (summary(logi_m4)[[4]]/summary(logi_m4)[[8]])
R_squared

#70/30 CV check

#Train
COBRE_train$prediction <- predict(logi_m4, COBRE_train, type = "response")


#Test
COBRE_test$prediction <- predict(logi_m4, COBRE_test, type = "response")

prop.table(table(COBRE$Subject_Type))


accuracy_info <- AccuracyCutoffInfo( train = COBRE_train, test = COBRE_test, 
                                     predict = "prediction", actual = "Subject_Type" )

accuracy_info$plot


Classify(COBRE_train, COBRE_train$prediction,"Subject_Type", 0.425)

Classify(COBRE_test, COBRE_test$prediction,"Subject_Type", 0.425)

```



```{r warning = FALSE}
#COBRE model k fold CV check

set.seed(4321)

Accuracy.k <- cv.error(COBRE_logi_subset, "Subject_Type", cut_off = 0.425)
Accuracy.k
mean(Accuracy.k)
```




```{r warning = FALSE}
#Combine data 

#Further data cleaning to merge CNP and COBRE data 
Study <- rep("CNP",nrow(CNP))

CNP <- data.frame(CNP,Study)

CNP <- CNP %>% 
  select(-c(7:41))

colnames(CNP)[5:6] <- c("Ethnicity","Education")

levels(CNP$Gender) <- c("Female","Male")

Study <- rep("COBRE",nrow(COBRE))
COBRE <- data.frame(COBRE,Study)

COBRE <- COBRE %>%
  select(-c(5,8:111))

# CNP Ethinicty
#1=Hispanic origin
#2=Not of Hispanic origin

#COBRE Ethinicty
#Caucasian = 1
#African-American	= 2
#Hispanic	= 3

#Recoding required

table(COBRE$Ethnicity)

for(i in 1:length(COBRE$Ethnicity)){
  if(!is.na(COBRE$Ethnicity[i])){
    if(COBRE$Ethnicity[i] == 1 | COBRE$Ethnicity[i] == 2)
      COBRE$Ethnicity[i] <- 4
  }
}
COBRE$Ethnicity <- COBRE$Ethnicity - 2


table(COBRE$Ethnicity)


Data <- merge(CNP,COBRE, all = TRUE) %>%
  select(-c(1))
Data$Ethnicity <- as.factor(Data$Ethnicity)

```


```{r}
set.seed(4321)

# Combine Data modeling


#Random Forest variable selection

rfsrc_m5 <- rfsrc(Study~.,data = Data, na.action = c("na.omit"), ntree= 1000)


max_var <- max.subtree(rfsrc_m5, conservative = TRUE)

max_var$topvars

#delete duplicate entity


subset <- as.vector(max_var$topvars)

subset <- delete_dup(subset,Data[,c(1:5,139:152)])


#Logistic Regression model

Data_logi <- Data[,c("Study",names(Data[,c(1:5,139:152)]), subset)]

Data_logi <- na.omit(Data_logi) %>%
  Standarize()
  

#check correlation

high_cor <- findCorrelation(cor(Data_logi[,-c(1,3:5)]),cutoff = 0.75) + 4

#Remove variables to prevent multicollinearity problem
Data_logi <- Data_logi %>%
  select(-c(high_cor))

index <- sample(1:nrow(Data_logi), size = round(nrow(Data_logi)*0.7,0),replace = FALSE)

Data_train <- Data_logi[index,]
Data_test <- Data_logi[-index,]
logi_m5 <-glm(Study~. + Subject_Type*Age , data = Data_train, family = "binomial") 
summary(logi_m5)
round(exp(coef(logi_m5)),3)
anova(logi_m5, test = "Chisq")


#R-squared

R_squared <- 1 - (summary(logi_m5)[[4]]/summary(logi_m5)[[8]])
R_squared

#Effect plot
plot(Effect(c("Subject_Type", "Age"), logi_m5),ask = FALSE)


#70/30 CV check

#Train
Data_train$prediction <- predict(logi_m5, Data_train, type = "response")


#Test
Data_test$prediction <- predict(logi_m5, Data_test, type = "response")

prop.table(table(COBRE$Subject_Type))


accuracy_info <- AccuracyCutoffInfo( train = Data_train, test = Data_test, 
                                     predict = "prediction", actual = "Study" )

accuracy_info$plot


Classify(Data_train, Data_train$prediction,"Study", 0.55)

Classify(Data_test, Data_test$prediction,"Study", 0.55)


```

```{r warning = FALSE}
#Combine data model k fold CV check
set.seed(4321)

Accuracy.k <- cv.error(Data_logi, "Study", cut_off = 0.55)
Accuracy.k
mean(Accuracy.k)
```